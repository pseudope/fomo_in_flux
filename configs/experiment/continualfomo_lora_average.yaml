# @package _global_
experiment:
  type: 'class_incremental'

  # Turn on open vocabulary classification style.
  training: 'contrastive'

  backbone:
    name: openclip_vit_b32
    cache_dir: './cache'
    head: default
    pretrained: True
    freeze_head: False
    freeze_features: False
    half_precision: True

  dataset:
    sequence: 'stream_construction/random_sequence_1.json'
    path: './data'
    pretraining_data_path: './data/laion400m/shards'
    preload: False
    create_resized_variant_if_possible: True
    num_workers: 8
    resize: 224
    img_size: 224
    train_transforms: [
      'RandomResizedCrop', 'ToTensor', 'Normalize'
    ]
    test_transforms: [
      'Resize','CenterCrop','ToTensor','Normalize'
    ]

  evaluation:
    batch_size: 512
    additional_datasets: [
      'caltech101',
      'caltech256',
      'cars196',
      'cifar10',
      'domainnet_quickdraw',
      'eurosat',
      'fashionmnist',
      'flickr30k',
      'food101',
      'gtsrb',
      'imagenet',
      'imagenet_a',
      'imagenet_d',
      'imagenet_r',
      'imagenet_s',
      'imagenet_v2',
      'mnist',
      'monkeys10',
      'mscoco',
      'oxford_pets',
      'stl10',
      'svhn'
    ]
    validate_on_subset: 0.1

  buffer:
    size: 500

  task:
    num: 20
    n_samples: 727040
    batch_size: 512
    eval_every_n_samples: 727040
    data_mixture:
      pretraining: 0.33
      update: 0.34
      buffer: 0.33

  optimizer:
    name: 'adamw'
    lr: 0.00001
    scaled_learning_rate: False
    loss: clip
    label_smoothing: 0
    weight_decay: 0.2
    clip_grad_norm: 1
    clip_temperature: 'learnable'

  scheduler:
    name: 'cosine_with_linear_warmup'
    cosine_lr_mul: 0.001
    warmup_perc: 0.1

log:
  group: openclip_vit_b32
  project: continualfomo_default
  checkpoint: False


continual:
  method: lora_per_task
  lora_per_task:
    max_logits_inference: False
